{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chollet's textbook was not clear on LSTMs. Therefore following textbook was used:\n",
    "\n",
    "### Natural Language Processing with TensorFlow by Thushan Ganegedara (Chapter 3, 4, 7, 8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **Word Embeddings**: Numerical feature representations for words. Ideally, good feature representations for words preserve their semantics (i.e. meaning) as well as their context in a language.\n",
    "2) **Word2vec** is a *neural-network* based technique to learn word embeddings or distributed numerical feature representations (that is, vectors) of words. Word2vec is called a *distributed representation*, as the semantics of the word is captured by the activation pattern of the full representation vector.\n",
    "3) One-hot encoding is also known as a *localist representation* (opposite of the distributed representation), as the feature representation is decided by the activation of a single element in the vector.\n",
    "4) Such learned word embeddings can be visualized using a t-SNE plot.\n",
    "\n",
    "\n",
    "\n",
    "## WordNet \n",
    "The NLTK library, a Python natural language processing library, can be used to understand WordNet and its mechanisms. Note that WordNets are available for multiple languages. (http://globalwordnet.org/resources/wordnets-in-the-world/)\n",
    "\n",
    "In WordNet, the word representations are modeled hierarchically i.e. synsets (group of words with similar meaning) can be related to each other. There are two main categories of these relations:\n",
    "* *is-a* association (Hypernyms, hyponyms)\n",
    "* *is-made-of* association (Holonyms, meronyms)\n",
    "\n",
    "![](synset.png)\n",
    "\n",
    "#### Problems with WordNet:\n",
    "1) Maintaining WordNet i.e. adding new synsets, definitions, lemmas, and so on, can be very expensive. This adversely affects the scalability of WordNet, as human labor is essential to keep WordNet up to date.\n",
    "2) depending on what you are trying to solve, WordNet might be suitable or you might be able to perform better with a loose definition of words\n",
    "3) Dependent on human's subjective understanding of language.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF method\n",
    "\n",
    "Extra Resource: https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/\n",
    "\n",
    "TF-IDF is a frequency-based method that takes into account the frequency with which a word appears in a corpus. Intuitively, the higher the frequency of the word, the more important that word is in the document. However, just calculating the frequency would not work, because words such as \"this\" and \"is\" are very frequent but do not carry that much information. TF-IDF takes this into consideration and gives a value of zero for such common words.\n",
    "\n",
    "* TF(`w`) = Term Frequency(`w`) = num. of times `w` appears/ total number of words\n",
    "* IDF(`w`) =  Inverse Document Frequency(`w`) = log(total number of documents / number of documents with `w` in it)\n",
    "* TF-IDF(`w`) = TF(`w`) x IDF(`w`)\n",
    "\n",
    "Key intuition motivating TF-IDF: the importance of a term is inversely related to its frequency across documents. TF gives us information on how often a term appears in a document and IDF gives us information about the relative rarity of a term in the collection of documents.\n",
    "\n",
    "![](tfdif.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "\n",
    "Word2vec is an approach that allows to learn the meaning of words without any human intervention. It learns numerical representations of words by looking at the words surrounding a given word i.e. the word's \"context\". By context, we refer to a fixed number of words after and before the word of interest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs (Ch. 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3cd3977e83b375402d32c0cbf11b3b0f3f7432503fe0f985eda8effc49de1364"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
